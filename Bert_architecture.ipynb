{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55efa23f-e6fa-4ce4-8017-5365bcbff3e1",
   "metadata": {},
   "source": [
    "# Bert Architecture and WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7633b-ed98-4f3c-9b35-780d5bd55273",
   "metadata": {},
   "source": [
    "BERT Architecture Overview\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model designed to pre-train deep bidirectional representations. Its architecture is based on the Transformer encoder, and the model is pre-trained on large amounts of text data using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). This allows BERT to understand the context of words in sentences both from the left and right directions.\n",
    "\n",
    "Input (Tokens) \n",
    "       |\n",
    "     [Embedding Layer]\n",
    "       |\n",
    "     [Transformer Encoder Layers]\n",
    "       |\n",
    "     [Final Embeddings]\n",
    "       |\n",
    "     [Prediction Heads] --> Masked Language Model (MLM)\n",
    "                         --> Next Sentence Prediction (NSP)\n",
    "\n",
    "\n",
    "- Input Layer: Token embeddings, segment embeddings, and position embeddings.\n",
    "- Transformer Encoder Layers: Multiple layers (typically 12 or 24) of self-attention and feed-forward layers.\n",
    "- Prediction Heads: Tasks like MLM and NSP use the final embeddings for prediction.\n",
    "\n",
    "\n",
    "##### BERT Workflow Explanation in Steps\n",
    "Let's go through the BERT workflow and understand it step-by-step using Python and Hugging Face's transformers library.\n",
    "\n",
    "1. Input Preprocessing: BERT requires input in the form of tokens, attention masks, and token type IDs. The BertTokenizer handles tokenization and preprocessing.\n",
    "2. Model Initialization: BERT’s pre-trained model is loaded using Hugging Face.\n",
    "3. Forward Pass: Input tokens are passed through the model to get final embeddings and outputs.\n",
    "4. Prediction Tasks: Use the outputs for downstream tasks (e.g., masked token prediction or sentence classification).\n",
    "\n",
    "\n",
    "Step-by-Step Breakdown:\n",
    "\n",
    "1. Tokenization:\n",
    "Text input is tokenized into input IDs, and an attention mask is created.\n",
    "The tokenizer adds special tokens ([CLS] at the start and [SEP] at the end of input).\n",
    "2. Embedding:\n",
    "Tokens are converted into embeddings. The embedding layer sums token embeddings, positional embeddings, and segment embeddings.\n",
    "3. Transformer Encoder:\n",
    "The core of BERT is the multi-layer transformer encoder. Each encoder layer applies multi-head attention and feed-forward networks to capture contextual information.\n",
    "4. Final Layer Output:\n",
    "The final output contains token-level embeddings (last_hidden_state) and the pooled_output for the [CLS] token, which can be used for classification tasks.\n",
    "5. Prediction:\n",
    "For masked language modeling, the final embeddings are used to predict the masked tokens. For sentence classification, the pooled_output is typically passed through a classification layer.\n",
    "\n",
    "    This flow provides a solid understanding of how BERT processes input text and generates contextualized embeddings for downstream tasks. You can further fine-tune the model for specific tasks like text classification, QA, etc.\n",
    "\n",
    "##### Detailed:\n",
    "1. Tokenization and Input Processing: How the tokenizer works, handling special tokens like [CLS] and [SEP], and token types (e.g., token type IDs for sentence pairs).\n",
    "2. Transformer Encoder Layer: Detailed breakdown of the self-attention mechanism, multi-head attention, and the role of positional embeddings.\n",
    "3. Masked Language Modeling (MLM): How BERT is pre-trained using MLM, where random tokens are masked and predicted.\n",
    "4. Next Sentence Prediction (NSP): How BERT is pre-trained to understand sentence relationships and the workflow for this task.\n",
    "5. Fine-Tuning for Downstream Tasks: How to fine-tune BERT for tasks like text classification, sentiment analysis, or question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6985b8a-9896-4387-ae0c-68cb4e734b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nares\\anaconda3\\lib\\site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nares\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23674c73-0586-4b59-b8b2-1e80a3fa5aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04e21c1-84ee-4de5-ac0e-e2e8971c8c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nares\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nares\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nares\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nares\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nares\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nares\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf533d-a2aa-4602-a02d-8c74dfb70166",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ba785e-7b0f-411c-97c4-3387e2ca593e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Tokenization and Input Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526eade-1ea7-4228-859b-7bb96b30e4d5",
   "metadata": {},
   "source": [
    "BERT’s tokenization process involves converting raw text into input tokens that the model can process. It uses a WordPiece Tokenizer, which splits words into subword units when necessary to handle rare or unknown words.\n",
    "\n",
    "Special Tokens: BERT uses special tokens like [CLS] for classification and [SEP] to separate sentences (or signal the end of a sentence).\n",
    "Token Type IDs: These indicate sentence A or sentence B for tasks involving sentence pairs (used in Next Sentence Prediction).\n",
    "Attention Masks: These tell the model which tokens to focus on (non-padding tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53b0fe3-7506-455c-9ed5-567d1505222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a28ab59-30df-486b-84a0-31e6d49ef614",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59fa4f52-e0db-4052-9bbe-b8036f916fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dda45825-9d53-4696-a935-95d55ad2654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text (with two sentences for NSP)\n",
    "text = \"BERT is a transformer model. It is used for various NLP tasks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7fdd3b53-b38c-49a9-84ed-d63d0a58fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input (convert to input IDs, attention masks, etc.)\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6c7cb98-3bb4-4581-b5f7-1a6b180d57a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs:  tensor([[  101, 14324,  2003,  1037, 10938,  2121,  2944,  1012,  2009,  2003,\n",
      "          2109,  2005,  2536, 17953,  2361,  8518,  1012,   102]])\n",
      "Attention Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Token Type IDs:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Input IDs: \", inputs['input_ids'])\n",
    "print(\"Attention Mask: \", inputs['attention_mask'])\n",
    "print(\"Token Type IDs: \", inputs['token_type_ids'])  # Used when handling sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9b190-a946-4378-a312-6c9913abea09",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "[CLS]: Added at the beginning for classification. \n",
    "\n",
    "[SEP]: Added at the end to separate sentences or mark the end of a single sentence.\n",
    "\n",
    "Padding: BERT uses fixed-length sequences, so shorter sequences are padded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e08d4a-75e0-4c91-ad84-fa0ffc2e6f0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e1198-e60d-4c6f-b9c7-9284cf4f7e6b",
   "metadata": {},
   "source": [
    "The transformer encoder layer is at the heart of BERT. It's a stack of layers that apply multi-head self-attention followed by position-wise feed-forward networks.\n",
    "\n",
    "Key Concepts:\n",
    "Self-Attention: BERT applies attention to each token by considering other tokens in the input sequence, allowing it to capture contextual relationships.\n",
    "Multi-Head Attention: BERT uses multiple attention heads to capture different aspects of context.\n",
    "Positional Embeddings: Since transformers don't have inherent knowledge of token order, BERT adds positional embeddings to the input.\n",
    "Self-Attention Formula:\n",
    "\n",
    "Query (Q), Key (K), and Value (V) vectors are created for each token.\n",
    "\n",
    "The attention scores are calculated as:\n",
    "\n",
    "Attention\n",
    "(\n",
    "𝑄\n",
    ",\n",
    "𝐾\n",
    ",\n",
    "𝑉\n",
    ")\n",
    "=\n",
    "softmax\n",
    "(\n",
    "𝑄\n",
    "𝐾\n",
    "𝑇\n",
    "𝑑\n",
    "𝑘\n",
    ")\n",
    "𝑉\n",
    "Attention(Q,K,V)=softmax(dkQKT)V\n",
    "\n",
    "where \n",
    "dk is the dimension of the key/query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe93b787-b145-417d-8bf2-add156122349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdf78ca-2606-49ca-aed9-2b54bc9edfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f88779c-2a04-4a06-aeb0-f874a267c129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6356, -0.5310, -0.0545,  ..., -0.2000, -0.1567,  0.8965],\n",
       "         [ 0.3849, -0.5095,  0.3477,  ..., -0.0883, -0.0330,  0.0359],\n",
       "         [-0.6935, -0.3289,  0.0345,  ..., -0.1874, -0.3868,  0.9165],\n",
       "         ...,\n",
       "         [-0.1377, -0.1032, -0.0988,  ..., -0.4231, -1.1112, -0.4846],\n",
       "         [ 0.4561, -0.1571, -0.4719,  ...,  0.2254, -0.4706, -0.2761],\n",
       "         [-0.7604, -0.3017,  0.3763,  ...,  0.6815, -0.6139,  0.5033]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7065e-01, -3.3561e-01, -8.6563e-01,  6.5553e-01,  7.4882e-01,\n",
       "         -2.0365e-01,  5.5993e-01,  1.7072e-01, -6.4585e-01, -9.9999e-01,\n",
       "         -3.5010e-01,  7.6290e-01,  9.6332e-01,  5.3351e-02,  7.4888e-01,\n",
       "         -6.5730e-01, -3.4508e-02, -4.6789e-01,  3.2335e-01,  6.8526e-02,\n",
       "          4.5708e-01,  9.9998e-01,  2.4238e-01,  2.6449e-01,  2.8081e-01,\n",
       "          8.8371e-01, -5.7988e-01,  8.0844e-01,  9.1552e-01,  7.5200e-01,\n",
       "         -4.0481e-01,  1.2540e-01, -9.7753e-01, -1.7450e-01, -9.3557e-01,\n",
       "         -9.8619e-01,  4.0013e-01, -7.2559e-01,  1.2494e-01, -6.1783e-02,\n",
       "         -8.7059e-01,  4.3507e-01,  9.9999e-01, -5.9652e-01,  4.2045e-01,\n",
       "         -1.4804e-01, -1.0000e+00,  2.6918e-01, -8.4587e-01,  8.2607e-01,\n",
       "          7.4814e-01,  7.6508e-01,  1.7666e-01,  4.1777e-01,  4.0145e-01,\n",
       "         -3.6069e-01, -2.6058e-01,  8.3721e-02, -2.4460e-01, -5.7878e-01,\n",
       "         -4.6777e-01,  1.6887e-01, -6.7983e-01, -8.9834e-01,  6.7776e-01,\n",
       "          5.8171e-01,  5.5375e-02, -2.7622e-01, -9.1965e-02, -2.4171e-02,\n",
       "          8.2086e-01,  1.5504e-01, -1.1275e-01, -6.7613e-01,  4.0632e-01,\n",
       "          2.3563e-01, -6.2623e-01,  1.0000e+00, -2.7973e-01, -9.5942e-01,\n",
       "          7.0768e-01,  4.9034e-01,  5.4577e-01, -2.2380e-02,  2.2173e-01,\n",
       "         -1.0000e+00,  6.4498e-01,  6.2690e-03, -9.7683e-01,  1.2136e-01,\n",
       "          5.6261e-01, -1.1722e-01,  2.9830e-01,  6.1666e-01, -3.0687e-01,\n",
       "         -4.0908e-01, -3.1021e-01, -8.5772e-01, -1.7536e-01, -4.0357e-01,\n",
       "          1.1686e-01, -2.0869e-01, -2.2579e-01, -3.3317e-01,  3.6407e-01,\n",
       "         -3.4014e-01, -8.5072e-02,  4.2079e-01, -1.4600e-01,  6.0053e-01,\n",
       "          4.8556e-01, -3.7028e-01,  2.7530e-01, -9.1062e-01,  4.7964e-01,\n",
       "         -3.5507e-01, -9.7910e-01, -5.9208e-01, -9.6913e-01,  7.2839e-01,\n",
       "          5.3844e-02, -1.6710e-01,  8.9622e-01,  3.2766e-02,  4.1291e-01,\n",
       "         -2.9132e-02, -8.5204e-01, -1.0000e+00, -5.6718e-01, -6.3108e-01,\n",
       "          1.5792e-01, -1.3998e-01, -9.6312e-01, -9.3165e-01,  6.1815e-01,\n",
       "          9.4204e-01,  1.5155e-01,  9.9993e-01, -1.9052e-01,  8.8002e-01,\n",
       "          1.7757e-01, -6.1822e-01,  3.8770e-01, -2.9495e-01,  6.8789e-01,\n",
       "         -2.0710e-01, -5.6311e-01,  1.5353e-01, -3.9545e-01,  2.3672e-01,\n",
       "         -5.4965e-01, -2.3738e-01, -6.6325e-01, -8.5563e-01, -2.7602e-01,\n",
       "          8.5127e-01, -5.4083e-01, -7.8824e-01,  2.5337e-01, -5.2200e-02,\n",
       "         -3.4012e-01,  8.4490e-01,  7.4143e-01,  1.7070e-01, -7.4658e-03,\n",
       "          2.9504e-01, -1.8462e-01,  4.8346e-01, -8.3801e-01, -6.9982e-03,\n",
       "          4.0289e-01, -2.8917e-01, -7.7835e-01, -9.6502e-01, -1.9798e-01,\n",
       "          5.2151e-01,  9.7007e-01,  6.0123e-01,  4.0166e-01,  6.7858e-01,\n",
       "         -3.2899e-01,  5.2352e-01, -9.1274e-01,  9.6998e-01, -9.8463e-03,\n",
       "          1.1976e-01, -5.5426e-01,  4.3355e-01, -8.8571e-01, -2.0793e-01,\n",
       "          7.2889e-01, -6.0402e-01, -8.5293e-01, -4.7376e-03, -5.1328e-01,\n",
       "         -3.2601e-01, -7.9737e-01,  6.6591e-01, -3.1179e-01, -3.0870e-01,\n",
       "          3.2538e-02,  8.6561e-01,  9.4230e-01,  6.4662e-01,  8.7885e-02,\n",
       "          6.2013e-01, -8.0005e-01,  4.0535e-02,  6.8858e-03,  1.9616e-01,\n",
       "          9.3188e-02,  9.8312e-01, -7.4198e-01, -9.6086e-02, -8.7499e-01,\n",
       "         -9.4939e-01, -1.4587e-01, -7.5922e-01, -7.5439e-02, -7.6261e-01,\n",
       "          5.5066e-01, -6.8624e-01, -8.8291e-03,  1.1641e-01, -7.9855e-01,\n",
       "         -5.8902e-01,  1.7220e-01, -4.8986e-01,  4.2323e-01, -2.0693e-01,\n",
       "          9.1021e-01,  9.0090e-01, -6.1691e-01,  1.4766e-01,  9.4470e-01,\n",
       "         -8.4739e-01, -8.0326e-01,  4.0844e-01, -2.4357e-01,  8.2147e-01,\n",
       "         -5.5094e-01,  9.9311e-01,  8.4410e-01,  4.3008e-01, -8.5238e-01,\n",
       "         -6.8041e-01, -8.7588e-01, -2.2708e-01, -2.2031e-03, -3.5978e-01,\n",
       "          6.9547e-01,  6.7921e-01,  1.9913e-01,  5.5244e-01, -4.5375e-01,\n",
       "          9.5316e-01, -9.2436e-01, -9.1962e-01, -8.6216e-01,  1.9624e-01,\n",
       "         -9.8214e-01,  7.8346e-01,  1.7822e-01,  7.5019e-01, -4.6725e-01,\n",
       "         -5.7387e-01, -9.2143e-01,  8.1090e-01,  8.8666e-02,  9.5971e-01,\n",
       "         -5.0841e-01, -6.8625e-01, -6.7525e-01, -8.6269e-01, -2.3669e-01,\n",
       "         -8.0506e-02,  2.8020e-02, -1.1811e-01, -8.9746e-01,  3.8189e-01,\n",
       "          5.9712e-01,  3.9977e-01, -6.5899e-01,  9.8024e-01,  1.0000e+00,\n",
       "          9.5530e-01,  7.6878e-01,  6.5254e-01, -9.9982e-01, -8.9262e-01,\n",
       "          1.0000e+00, -9.8257e-01, -1.0000e+00, -9.0222e-01, -6.3617e-01,\n",
       "          2.1553e-01, -1.0000e+00, -4.1545e-02,  9.1362e-02, -8.6274e-01,\n",
       "          4.6185e-01,  9.5264e-01,  8.6121e-01, -1.0000e+00,  9.3212e-01,\n",
       "          8.8746e-01, -6.6270e-01,  7.3672e-01, -5.1295e-01,  9.4980e-01,\n",
       "          3.1464e-01,  5.8690e-01, -1.5012e-01,  2.2185e-01, -8.6584e-01,\n",
       "         -7.4350e-01, -4.1601e-01, -7.0863e-01,  9.9883e-01,  6.6547e-02,\n",
       "         -7.4795e-01, -7.6342e-01,  5.9198e-01,  1.1751e-01, -2.2931e-01,\n",
       "         -9.1587e-01, -2.5813e-01,  1.5860e-01,  7.3591e-01,  2.5195e-01,\n",
       "          2.5003e-01, -6.4717e-01,  3.5280e-01, -3.4392e-02,  1.5667e-01,\n",
       "          6.4756e-01, -9.0513e-01, -3.8297e-01, -3.0340e-02, -9.3217e-02,\n",
       "         -2.6992e-01, -9.5634e-01,  9.2943e-01, -3.4429e-01,  7.8754e-01,\n",
       "          1.0000e+00,  7.3351e-01, -7.4770e-01,  4.8838e-01,  2.2252e-01,\n",
       "         -3.2812e-01,  1.0000e+00,  6.8471e-01, -9.6583e-01, -5.7620e-01,\n",
       "          6.7010e-01, -5.1281e-01, -5.8690e-01,  9.9956e-01, -2.0172e-01,\n",
       "         -5.7052e-01, -3.1488e-01,  9.6543e-01, -9.7793e-01,  9.9510e-01,\n",
       "         -6.8451e-01, -9.3008e-01,  9.2908e-01,  8.4922e-01, -4.9502e-01,\n",
       "         -8.9036e-01, -1.6812e-02, -4.9742e-01,  2.5384e-01, -8.7164e-01,\n",
       "          5.2984e-01,  3.3727e-01, -4.3010e-02,  7.8950e-01, -5.8170e-01,\n",
       "         -6.2469e-01,  3.6433e-01, -5.2877e-01,  1.0895e-01,  9.2159e-01,\n",
       "          4.5295e-01, -2.9265e-01, -1.1637e-01, -1.3431e-01, -6.7972e-01,\n",
       "         -9.4165e-01,  5.3712e-01,  1.0000e+00, -1.5205e-02,  7.6325e-01,\n",
       "          4.2636e-02,  1.5394e-03, -1.6929e-01,  5.2735e-01,  5.5769e-01,\n",
       "         -1.7156e-01, -8.5338e-01,  6.2698e-01, -6.4303e-01, -9.7673e-01,\n",
       "          4.3896e-01,  2.8978e-02, -1.8151e-01,  9.9991e-01,  1.7696e-01,\n",
       "          2.2568e-01,  3.8180e-01,  9.4260e-01, -4.4805e-02,  4.2677e-01,\n",
       "          5.5690e-01,  9.6358e-01, -1.7362e-01,  6.2252e-01,  6.8453e-01,\n",
       "         -6.3577e-01, -1.0891e-01, -6.2114e-01, -3.3631e-02, -8.9886e-01,\n",
       "          1.3207e-01, -9.1158e-01,  9.3776e-01,  9.0594e-01,  2.8431e-01,\n",
       "          1.5089e-01,  7.4120e-01,  1.0000e+00, -9.3873e-01,  4.2054e-01,\n",
       "          2.8096e-01,  2.1277e-01, -9.9981e-01, -4.2170e-01, -2.5980e-01,\n",
       "         -8.1023e-02, -5.8213e-01, -2.1786e-01,  9.4181e-02, -9.5286e-01,\n",
       "          5.2283e-01,  5.3384e-01, -8.9842e-01, -9.6723e-01, -1.6456e-01,\n",
       "          6.8191e-01, -2.6810e-02, -9.8580e-01, -3.6299e-01, -5.8134e-01,\n",
       "          4.2178e-01, -2.2185e-01, -7.9685e-01,  1.5893e-01, -2.8465e-01,\n",
       "          4.1098e-01, -3.5137e-01,  6.2838e-01,  7.2226e-01,  8.7302e-01,\n",
       "         -9.2336e-01, -2.6350e-01, -2.9504e-02, -6.6956e-01,  6.9609e-01,\n",
       "         -7.8497e-01, -8.2019e-01, -1.1547e-01,  1.0000e+00, -2.6864e-01,\n",
       "          8.0198e-01,  5.7883e-01,  6.1725e-01, -1.2196e-01,  1.0677e-01,\n",
       "          9.1462e-01,  1.2967e-01, -1.9854e-01, -8.2167e-01,  2.0626e-01,\n",
       "         -3.4492e-01,  5.6136e-01,  6.8494e-01,  4.7813e-01,  6.3843e-01,\n",
       "          8.6748e-01,  7.3189e-02,  1.2413e-01, -1.3426e-01,  9.8087e-01,\n",
       "         -1.2998e-01, -1.8452e-01, -4.3221e-01,  6.2223e-02, -2.3520e-01,\n",
       "          2.2715e-01,  1.0000e+00,  2.7295e-01,  6.1971e-01, -9.7523e-01,\n",
       "         -7.6828e-01, -8.9632e-01,  1.0000e+00,  7.6989e-01, -8.8452e-01,\n",
       "          6.4315e-01,  5.1777e-01, -6.2194e-02,  6.0300e-01, -2.7669e-01,\n",
       "         -2.0770e-01,  1.3992e-01,  3.8642e-02,  9.0055e-01, -3.0711e-01,\n",
       "         -9.5324e-01, -7.3424e-01,  3.8796e-01, -9.3772e-01,  9.9992e-01,\n",
       "         -5.7362e-01, -2.1827e-01, -2.9160e-01, -4.0300e-01, -5.0435e-01,\n",
       "         -8.7772e-02, -9.5313e-01, -1.1964e-02,  8.5549e-02,  9.1195e-01,\n",
       "          3.3251e-01, -5.7927e-01, -8.3919e-01,  8.3482e-01,  7.4736e-01,\n",
       "         -7.2912e-01, -9.2391e-01,  9.1472e-01, -9.2085e-01,  4.0689e-01,\n",
       "          1.0000e+00,  4.5037e-01,  1.4970e-01,  4.5711e-02, -3.2293e-01,\n",
       "          2.5654e-01, -5.2443e-01,  3.9645e-01, -9.2131e-01, -1.8465e-01,\n",
       "         -1.2331e-01,  3.5924e-01, -1.1876e-01, -6.9227e-01,  5.1105e-01,\n",
       "          1.1516e-01, -5.5570e-01, -5.0227e-01,  2.8627e-02,  3.3945e-01,\n",
       "          7.0484e-01, -9.3833e-02, -9.5504e-02,  1.4278e-01, -1.1057e-02,\n",
       "         -6.7220e-01, -3.1261e-01, -4.2142e-01, -9.9999e-01,  6.2114e-01,\n",
       "         -1.0000e+00,  5.4090e-01, -1.4634e-01, -1.2906e-01,  7.8822e-01,\n",
       "          7.1886e-01,  6.2580e-01, -6.2092e-01, -6.0215e-01,  6.2091e-01,\n",
       "          4.9183e-01, -3.3990e-01, -1.6870e-01, -4.9757e-01,  2.0127e-01,\n",
       "          1.5292e-01,  1.0196e-01, -3.9891e-01,  7.0510e-01, -1.6572e-01,\n",
       "          1.0000e+00, -5.2316e-04, -5.9327e-01, -7.5014e-01,  2.1730e-01,\n",
       "         -2.1115e-01,  1.0000e+00, -7.5138e-01, -9.3714e-01,  1.9734e-01,\n",
       "         -7.4340e-01, -7.2232e-01,  3.5843e-01,  6.7183e-02, -7.3897e-01,\n",
       "         -8.8886e-01,  8.2735e-01,  7.6066e-01, -6.1072e-01,  3.9418e-01,\n",
       "         -1.5772e-01, -4.4147e-01, -7.4453e-02,  9.0490e-01,  9.7278e-01,\n",
       "          6.3032e-01,  8.4395e-01, -5.0117e-01, -1.9836e-01,  9.3742e-01,\n",
       "          2.4061e-01,  2.0480e-01, -4.0066e-02,  1.0000e+00,  3.5269e-01,\n",
       "         -8.6801e-01,  3.3987e-01, -9.4970e-01, -1.0798e-01, -9.1998e-01,\n",
       "          2.0304e-01,  6.7623e-02,  8.6382e-01, -2.7242e-01,  9.0171e-01,\n",
       "         -8.0078e-01, -4.8579e-02, -5.4368e-01, -2.5322e-01,  2.2998e-01,\n",
       "         -8.8906e-01, -9.6678e-01, -9.7105e-01,  5.5649e-01, -3.3343e-01,\n",
       "         -1.5176e-01,  1.2743e-01,  4.4456e-02,  3.5981e-01,  2.7270e-01,\n",
       "         -1.0000e+00,  9.2440e-01,  3.9810e-01,  6.5901e-01,  9.0769e-01,\n",
       "          7.1685e-01,  6.4176e-01,  2.6781e-01, -9.7460e-01, -7.4366e-01,\n",
       "         -1.8850e-01, -3.2488e-01,  6.8196e-01,  6.3476e-01,  6.9240e-01,\n",
       "          1.8039e-01, -4.2031e-01, -5.4681e-01, -1.9959e-01, -8.6983e-01,\n",
       "         -9.8342e-01,  3.2565e-01, -3.3475e-01, -7.5230e-01,  9.2345e-01,\n",
       "         -4.9235e-01, -3.2371e-04,  3.8388e-01, -7.6255e-01,  7.5544e-01,\n",
       "          8.4549e-01,  1.5981e-01,  2.5713e-02,  5.6432e-01,  7.8801e-01,\n",
       "          8.9371e-01,  9.7135e-01, -7.3939e-01,  6.8583e-01, -6.4136e-01,\n",
       "          4.1212e-01,  9.0091e-01, -9.5016e-01,  1.2970e-01,  5.5025e-01,\n",
       "          9.1510e-02,  1.2395e-01, -1.0693e-01, -7.4757e-01,  8.8965e-01,\n",
       "         -1.8919e-01,  4.8336e-01, -3.4265e-01,  1.7230e-01, -3.9469e-01,\n",
       "         -1.8537e-01, -8.1017e-01, -5.2230e-01,  5.4391e-01,  1.5371e-01,\n",
       "          7.9676e-01,  8.2868e-01,  5.5624e-02, -5.8514e-01, -3.3203e-02,\n",
       "         -4.9307e-01, -9.0780e-01,  7.6500e-01,  5.0793e-02,  2.0925e-01,\n",
       "          7.6279e-01, -1.4254e-01,  9.7331e-01,  3.7488e-02, -3.5802e-01,\n",
       "         -1.6699e-01, -7.4815e-01,  7.9104e-01, -7.7009e-01, -5.0794e-01,\n",
       "         -4.9131e-01,  7.0873e-01,  2.9288e-01,  9.9999e-01, -6.3107e-01,\n",
       "         -6.5154e-01, -5.4373e-01, -3.1923e-01,  3.8886e-01, -3.8444e-01,\n",
       "         -1.0000e+00,  4.2819e-01, -3.9478e-01,  4.4614e-01, -6.0054e-01,\n",
       "          7.4951e-01, -5.6282e-01, -9.1963e-01, -2.4486e-01,  6.8285e-01,\n",
       "          7.0190e-01, -5.1162e-01, -5.8037e-01,  5.3711e-01, -2.4936e-01,\n",
       "          9.4486e-01,  7.4573e-01, -4.1530e-01,  4.0884e-01,  6.3837e-01,\n",
       "         -8.8947e-01, -5.5853e-01,  7.6069e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass through BERT\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "beb123bc-f45a-4c91-89a9-b186b86c28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last hidden states (for each token) and pooled output (for classification)\n",
    "last_hidden_state = outputs.last_hidden_state # Token-level embeddings\n",
    "pooler_output = outputs.pooler_output ## Embbeding for [CLS} token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5207d7f-ddde-4beb-a7cd-3b1e266c95da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Shape:  torch.Size([1, 18, 768])\n",
      "Pooled Output Shape:  torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last Hidden State Shape: \", last_hidden_state.shape)\n",
    "print(\"Pooled Output Shape: \", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cf108-6708-402a-9f4b-7eac2c55ee66",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "###### Multi-Head Self-Attention allows BERT to look at each token from different perspectives.\n",
    "###### Feed-forward layers follow attention, applying non-linearity and projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e444e-7f24-4f16-9fee-a54116a6f6e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Masked Language Modeling (MLM)\n",
    "\n",
    "During pre-training, BERT randomly masks some input tokens (typically 15%) and trains the model to predict them. This allows BERT to learn bidirectional context.\n",
    "\n",
    "###### Masking Strategy: \n",
    "80% of the time, the token is replaced with [MASK], \n",
    "10% is replaced with a random token, and 10% is left unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0525b1e-10a5-4492-adfe-89cda7fd348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "046d1b22-4fc7-4d4d-8d56-d18e44046543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## Load pre-trained model for MLM\n",
    "model_mlm = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "92f38962-d327-48be-9b96-0a4f3f55bec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 14324,  2003,   103,  1037,  2944,  2005, 17953,  2361,  8518,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example sentences with [MASK] token\n",
    "masked_text = \"BERT is [MASK] a model for NLP tasks.\"\n",
    "inputs = tokenizer(masked_text, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7a199bfb-817b-4412-a74a-e771cbf1e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -6.5931,  -6.5281,  -6.5194,  ...,  -5.8572,  -5.6498,  -4.1681],\n",
       "         [ -5.9752,  -5.9742,  -5.9816,  ...,  -6.0757,  -5.9236,  -4.5834],\n",
       "         [-10.8940, -10.3371, -10.8128,  ...,  -9.9103,  -8.2486,  -8.0948],\n",
       "         ...,\n",
       "         [ -0.7863,  -0.4980,  -0.6281,  ...,  -1.3897,  -1.0887,  -2.6577],\n",
       "         [-11.2825, -10.9634, -11.0110,  ...,  -8.5080,  -8.9784,  -7.6255],\n",
       "         [-15.6298, -15.5663, -15.5410,  ..., -14.5949, -12.5687,  -9.8353]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Forword pass for MLM\n",
    "outputs = model_mlm(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f4a3d33f-6a93-426c-8042-96d6f4e1c42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.5931,  -6.5281,  -6.5194,  ...,  -5.8572,  -5.6498,  -4.1681],\n",
       "         [ -5.9752,  -5.9742,  -5.9816,  ...,  -6.0757,  -5.9236,  -4.5834],\n",
       "         [-10.8940, -10.3371, -10.8128,  ...,  -9.9103,  -8.2486,  -8.0948],\n",
       "         ...,\n",
       "         [ -0.7863,  -0.4980,  -0.6281,  ...,  -1.3897,  -1.0887,  -2.6577],\n",
       "         [-11.2825, -10.9634, -11.0110,  ...,  -8.5080,  -8.9784,  -7.6255],\n",
       "         [-15.6298, -15.5663, -15.5410,  ..., -14.5949, -12.5687,  -9.8353]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "39bddcf4-1b04-405f-888f-2bbe66be938a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 14324,  2003,   103,  1037,  2944,  2005, 17953,  2361,  8518,\n",
       "          1012,   102]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1576f474-2fab-45a6-bb32-b1e20c60730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "aa15475a-c3cd-4e37-ad8b-145e5270e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "05b090d1-4d1c-4a4d-8b7c-e3599af0e52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "masked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a72ce662-2faf-4231-b90f-b303bbb8ac16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2036])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_id = logits[0, masked_index].argmax(axis = 1)\n",
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "66b6ff07-69c0-43a4-89bd-0ddfa932dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token for [MASK]: also\n"
     ]
    }
   ],
   "source": [
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(f\"Predicted token for [MASK]: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44c8c2-644e-41e1-8b08-cbc2f2031452",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "###### MLM Pre-training: \n",
    "This task teaches BERT to predict missing tokens, which enhances its understanding of the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8a747-af6a-4368-af2b-267ff0f62705",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Next Sentence Prediction(NSP)\n",
    "BERT is also pre-trained on a binary classification task called Next Sentence Prediction (NSP). Given two sentences, it predicts whether the second sentence follows the first one in the original text.\n",
    "\n",
    "50% of the time, the second sentence is the next sentence.\n",
    "50% of the time, it’s a random sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d5535831-00a5-4989-80e1-4707ef208ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f2039e96-fb7f-48c3-b909-7ebcca6b9cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load pre-trained model for NSP\n",
    "model_nsp = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "model_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ad657cc3-9c30-4085-9283-07ceb32c5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two sentences for NSP\n",
    "sentence_1 = \"BERT is a transformer model.\"\n",
    "sentence_2 = \"It is used for various NLP tasks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f2b6bce3-77d5-4555-b617-1e0d9aadc18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 14324,  2003,  1037, 10938,  2121,  2944,  1012,   102,  2009,\n",
       "          2003,  2109,  2005,  2536, 17953,  2361,  8518,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence pair for NSP task\n",
    "inputs = tokenizer(sentence_1, sentence_2, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "496b550d-6831-483a-bd64-13b5f67ea8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSentencePredictorOutput(loss=None, logits=tensor([[ 5.8095, -5.0802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Forword pass for NSP\n",
    "outputs = model_nsp(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c0239321-1e12-427f-acd7-424efc07471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8095, -5.0802]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d2839d7a-94f6-4bf5-a24a-eca92f3b04f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSP logits:  tensor([[ 5.8095, -5.0802]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Logits indicate the probability of next sentence\n",
    "print(\"NSP logits: \", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb2826-aa8f-439e-ab64-7044bb358d0c",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "###### NSP Pre-training: \n",
    "This task helps BERT understand sentence relationships, which is useful in tasks like question answering and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdabd3d-36c4-4a0d-a435-37772c560663",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Fine-Tuning for Downstream Tasks\n",
    "BERT can be fine-tuned for a variety of NLP tasks, such as text classification, question answering, and named entity recognition. Fine-tuning is typically done by adding a task-specific head (e.g., classification layer) on top of BERT and training the model on a specific dataset.\n",
    "\n",
    "Example: Fine-tuning for Text Classification\n",
    "\n",
    "1. Add a classification head (e.g., a simple linear layer) on top of the pooled output from BERT.\n",
    "2. Train on a labeled dataset (e.g., sentiment analysis) by minimizing a loss function (e.g., cross-entropy loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7263dda6-4318-461c-a365-cd3c09b86854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "56754248-a97f-4fc3-9078-2b98f13a1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained BERT model for text classification (binary classification)\n",
    "model_cls = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2aab785a-e9d8-4c7f-ae47-c75807ae37e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 14324,  2003, 12476,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input\n",
    "tokenizer(\"BERT is awesome!\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b36f0d1c-714e-419e-ac57-a3b8189c7f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.2670, 0.0884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass (get classification logits)\n",
    "outputs = model_cls(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c6e35adc-cad5-4578-861a-9dbeb337de7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2670, 0.0884]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "022558f7-3949-4e4b-aad2-bbf465d27f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute loss (if labels are provided)\n",
    "labels = torch.tensor([1]).unsqueeze(0) # Batch size 1, binary label\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ac3d7746-6a10-4b47-925e-834b449ed631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for text classification (binary classification)\n",
    "model_cls = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Example input\n",
    "inputs = tokenizer(\"BERT is awesome!\", return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9ff1301b-c04d-42d7-a662-9165ac71d517",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[306], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Compute loss (if labels are provided)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Batch size 1, binary label\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, labels)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1189\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1190\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward pass (get classification logits)\n",
    "outputs = model_cls(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Compute loss (if labels are provided)\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1, binary label\n",
    "loss = torch.nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ba7c5db5-31b2-473a-99fa-e699c1c82558",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[307], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model_cls\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 4\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits: \u001b[39m\u001b[38;5;124m\"\u001b[39m, logits)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model_cls.parameters(), lr=1e-5)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Logits: \", logits)\n",
    "print(\"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a592ff-7bb5-40fd-b058-f88f36b602bf",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "1. Fine-Tuning: BERT is adaptable for many tasks by simply training a small classification layer on top of its embeddings.\n",
    "2. Transfer Learning: The fine-tuning process leverages BERT's pre-trained knowledge and adapts it to specific tasks with minimal training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70378929-2a71-4f05-8b55-035895c4bb9b",
   "metadata": {},
   "source": [
    "##### Summary of BERT Workflow:\n",
    "1. Tokenization: Convert input text into tokens, including special tokens [CLS] and [SEP].\n",
    "2. Transformer Encoding: Apply multi-head attention and feed-forward networks to create contextual embeddings.\n",
    "3. Pre-training Tasks: MLM and NSP help BERT learn language representation.\n",
    "4. Fine-Tuning: Adapt BERT for specific NLP tasks with minimal modifications to the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca08d2c-d730-4327-8b0d-23156d8d6259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
